{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC2QnhmKxpq1"
   },
   "source": [
    "# Module 3: Data Ingestion with dlt (Data Load Tool)\n",
    "**Description:** Discovery and Prototyping for 2024 Yellow Taxi Ingestion.\n",
    "**Course:** DataTalksClub Data Engineering Zoomcamp (2026)\n",
    "\n",
    "**Credits:** * **Original dlt logic:** DataTalksClub / dltHub\n",
    "* **Modifications & Infrastructure Sync:** Victoria T.\n",
    "* **Key Modification:** Replaced manual secrets with local `terraform.tfvars` parsing and ADC authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hcl2\n",
    "import dlt\n",
    "from dlt.destinations import filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Terraform config\n",
    "def get_tf_config(path=\"../terraform/terraform.tfvars\"):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            config = hcl2.load(f)\n",
    "            # standard hcl2 extraction\n",
    "            return {k: v[0] if isinstance(v, list) else v for k, v in config.items()}\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: terraform.tfvars not found. Falling back to local defaults.\")\n",
    "        return {}\n",
    "\n",
    "CONFIG = get_tf_config()\n",
    "BUCKET_NAME = CONFIG.get(\"gcs_bucket_name\", \"your-fallback-bucket\")\n",
    "PROJECT_ID = CONFIG.get(\"project\", \"your-fallback-project\")\n",
    "\n",
    "# set dlt environment variables (use 'gcloud login' session)\n",
    "os.environ[\"DESTINATION__FILESYSTEM__BUCKET_URL\"] = f\"gs://{BUCKET_NAME}\"\n",
    "os.environ[\"DESTINATION__BIGQUERY__PROJECT_ID\"] = PROJECT_ID\n",
    "\n",
    "print(f\"dlt linked to Bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76zT1PzAgs7A"
   },
   "source": [
    "Ingesting parquet files to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.source(name=\"rides\")\n",
    "def download_parquet():\n",
    "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024\"\n",
    "    for month in range(1, 7):\n",
    "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
    "        url = f\"{prefix}-0{month}.parquet\"\n",
    "        \n",
    "        print(f\"Downloading: {url}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            df = pd.read_parquet(BytesIO(response.content))\n",
    "            # return the dataframe as a dlt resource for ingestion\n",
    "            yield dlt.resource(df, name=file_name)\n",
    "\n",
    "# initialize GCS Pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline_gcs\",\n",
    "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
    "    dataset_name=\"rides_dataset\",\n",
    ")\n",
    "\n",
    "# run the pipeline to load Parquet data into DuckDB\n",
    "load_info = pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0310FT-gy_P"
   },
   "source": [
    "Ingesting data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsUZobVduL7l"
   },
   "outputs": [],
   "source": [
    "# Re-using the resource for local testing\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def download_parquet_single():\n",
    "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024'\n",
    "    for month in range(1, 7):\n",
    "        url = f\"{prefix}-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            yield pd.read_parquet(BytesIO(response.content))\n",
    "\n",
    "pipeline_test = dlt.pipeline(\n",
    "    pipeline_name=\"rides_test_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"rides_test_dataset\",\n",
    ")\n",
    "\n",
    "info = pipeline_test.run(download_parquet_single)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDcLjzLtooBV",
    "outputId": "74ff2de7-2f2e-41b9-a681-3dc5887f6eed"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# connect to the database file\n",
    "conn = duckdb.connect(f\"{pipeline_test.pipeline_name}.duckdb\")\n",
    "\n",
    "# use the dynamic dataset_name from the pipeline object\n",
    "# dlt often prefixes or modifies this name internally\n",
    "dataset_schema = pipeline_test.dataset_name\n",
    "\n",
    "try:\n",
    "    conn.sql(f\"SET search_path = '{dataset_schema}'\")\n",
    "    print(f\"Search path set to: {dataset_schema}\")\n",
    "    \n",
    "    # describe the dataset to see the 'rides' table\n",
    "    res = conn.sql(\"DESCRIBE\").df()\n",
    "    print(res)\n",
    "except duckdb.CatalogException:\n",
    "    print(f\"'{dataset_schema}' not found. Available schemas:\")\n",
    "    print(conn.sql(\"SELECT schema_name FROM information_schema.schemata\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVJy8JoerI2P",
    "outputId": "3f8c7fee-a9ee-4fd4-ec75-153ca60bd36f"
   },
   "outputs": [],
   "source": [
    "# provide a resource name to query a table of that name\n",
    "with pipeline_test.sql_client() as client:\n",
    "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
    "        data = cursor.df()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
